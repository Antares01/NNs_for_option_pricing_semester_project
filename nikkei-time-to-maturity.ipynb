{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-28T14:49:46.662602Z","iopub.execute_input":"2022-06-28T14:49:46.663729Z","iopub.status.idle":"2022-06-28T14:49:46.697799Z","shell.execute_reply.started":"2022-06-28T14:49:46.663628Z","shell.execute_reply":"2022-06-28T14:49:46.696861Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nfrom scipy.stats import lognorm\nfrom matplotlib import pyplot as plt\nfrom scipy.ndimage.filters import uniform_filter1d\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom datetime import date, datetime\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:49:46.699414Z","iopub.execute_input":"2022-06-28T14:49:46.699922Z","iopub.status.idle":"2022-06-28T14:49:49.951062Z","shell.execute_reply.started":"2022-06-28T14:49:46.699892Z","shell.execute_reply":"2022-06-28T14:49:49.949676Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#integration parameter\nINTERVAL_LENGTH = 10\nSTART_UNIF = 0.01\nSCALING_CONSTANT = 10000","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:49:49.953089Z","iopub.execute_input":"2022-06-28T14:49:49.954196Z","iopub.status.idle":"2022-06-28T14:49:49.959715Z","shell.execute_reply.started":"2022-06-28T14:49:49.954144Z","shell.execute_reply":"2022-06-28T14:49:49.957935Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"options = pd.read_csv(\"../input/jpx-tokyo-stock-exchange-prediction/train_files/options.csv\")\nprint(options.columns)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:59:52.548027Z","iopub.execute_input":"2022-06-28T14:59:52.548921Z","iopub.status.idle":"2022-06-28T15:00:15.674212Z","shell.execute_reply.started":"2022-06-28T14:59:52.548875Z","shell.execute_reply":"2022-06-28T15:00:15.673371Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3552: DtypeWarning: Columns (7,8,9,10) have mixed types.Specify dtype option on import or set low_memory=False.\n  exec(code_obj, self.user_global_ns, self.user_ns)\n","output_type":"stream"},{"name":"stdout","text":"Index(['DateCode', 'Date', 'OptionsCode', 'WholeDayOpen', 'WholeDayHigh',\n       'WholeDayLow', 'WholeDayClose', 'NightSessionOpen', 'NightSessionHigh',\n       'NightSessionLow', 'NightSessionClose', 'DaySessionOpen',\n       'DaySessionHigh', 'DaySessionLow', 'DaySessionClose', 'TradingVolume',\n       'OpenInterest', 'TradingValue', 'ContractMonth', 'StrikePrice',\n       'WholeDayVolume', 'Putcall', 'LastTradingDay', 'SpecialQuotationDay',\n       'SettlementPrice', 'TheoreticalPrice', 'BaseVolatility',\n       'ImpliedVolatility', 'InterestRate', 'DividendRate', 'Dividend'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"datasets = []\nfor trading_date in tqdm(options['Date'].unique()):\n    dataset = options.loc[options['Date'] == trading_date]\n    for option_expiry in dataset['LastTradingDay'].unique():\n        dataset = dataset.loc[(options['LastTradingDay'] == option_expiry)& (options['WholeDayVolume']> 0)][['StrikePrice', 'Putcall', 'SettlementPrice']]#[['WholeDayClose', 'SettlementPrice']]\n        if dataset.empty:\n            continue\n\n        dataset[['StrikePrice', 'SettlementPrice']] = dataset[['StrikePrice', 'SettlementPrice']].transform(lambda x : x/SCALING_CONSTANT)\n        dataset['Putcall'] = dataset['Putcall'].transform(lambda x : False if x==1 else True)\n        trading_date = datetime.strptime(trading_date, '%Y-%m-%d')\n        option_expiry = datetime.strptime(str(option_expiry), '%Y%m%d')\n        ttm = option_expiry - trading_date\n        datasets.append((dataset, ttm.days))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T15:03:57.485937Z","iopub.execute_input":"2022-06-28T15:03:57.486407Z","iopub.status.idle":"2022-06-28T15:19:09.031228Z","shell.execute_reply.started":"2022-06-28T15:03:57.486364Z","shell.execute_reply":"2022-06-28T15:19:09.030013Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 1202/1202 [15:11<00:00,  1.32it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\n# Step 2\nwith open('/kaggle/working/datasets.pkl', 'wb') as datasets_file:\n \n  # Step 3\n  pickle.dump(datasets, datasets_file)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-28T15:19:34.724314Z","iopub.execute_input":"2022-06-28T15:19:34.724704Z","iopub.status.idle":"2022-06-28T15:19:34.759346Z","shell.execute_reply.started":"2022-06-28T15:19:34.724674Z","shell.execute_reply":"2022-06-28T15:19:34.757927Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"ttms = [dataset[1] for dataset in datasets]\nlengths = [len(dataset[0]) for dataset in datasets]","metadata":{"execution":{"iopub.status.busy":"2022-06-28T15:37:31.102179Z","iopub.execute_input":"2022-06-28T15:37:31.102588Z","iopub.status.idle":"2022-06-28T15:37:31.109399Z","shell.execute_reply.started":"2022-06-28T15:37:31.102555Z","shell.execute_reply":"2022-06-28T15:37:31.108094Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\n\nimport pickle\nwith open('/kaggle/working/datasets.pkl', 'rb') as datasets_file:\n    datasets_copy = pickle.load(datasets_file)\n\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-06-28T14:49:50.155178Z","iopub.status.idle":"2022-06-28T14:49:50.155747Z","shell.execute_reply.started":"2022-06-28T14:49:50.155446Z","shell.execute_reply":"2022-06-28T14:49:50.155474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TimeEncodingNet(nn.Module):\n    def __init__(out_size, self):\n        self.out_size = out_size\n        self.mlp = nn.Sequential(\n            nn.Linear(2, 16),\n            nn.ReLU(),\n            nn.Linear(16, 32),\n            nn.ReLU(),\n            nn.Linear(32, out_size)\n        )\n        \n    def forward(self, ttm):\n        return self.mlp((ttm, torch.sqrt(ttm) ) )\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImprovedNet(nn.Module):\n    def __init__(time_encoding_net, self):\n        super(BaselineNet, self).__init__()\n        self.time_encoding_net = time_encoding_net\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(2, 128),\n            nn.LeakyReLU(1e-1),\n            \n            nn.Linear(128, 512),\n            nn.LeakyReLU(1e-1),   \n\n            nn.Linear(512, 128),\n            nn.LeakyReLU(1e-1),\n            \n            nn.Linear(128, self.time_encoding_net.out_size)\n        )\n        self.common_head = nn.Sequential(\n            nn.Linear(2*self.time_encoding_net.out_size, 64),\n            nn.LeakyReLU(1e-1),\n            nn.Linear(64, 1)\n        )\n        self.softplus = nn.Softplus()\n        \n        \n    def forward(self, x, ttm):\n        x = self.flatten(x)\n        y = torch.log(x)\n        x = self.linear_relu_stack(torch.cat((x,y), 1))\n        x = torch.cat((x, self.time_encoding_net(ttm)), 1)\n        x = self.common_head(x)\n        x = self.softplus(x)\n        return x","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class OptionLoss(nn.Module):\n    def __init__(self):\n        super(OptionLoss,self).__init__()\n        self.huber = nn.HuberLoss()\n    \n    # outputs the MC_STEPS predictions, labels the TOT_OPTIONS labels\n    def forward(self, outputs, labels, strike, is_call, coordinates):\n        mc_steps = len(coordinates)\n        loss = 0\n        mc_integral = 0\n        payoff = lambda  j :  torch.max(torch.zeros(len(outputs)), coordinates - strike[j] ) if is_call[j] else torch.max(torch.zeros(len(outputs)), strike[j] - coordinates)\n        density = 1/INTERVAL_LENGTH \n        mc_prices = torch.stack([ torch.sum(payoff(j) * outputs / density)/mc_steps for j in range(len(labels))])\n        \n        return self.huber(mc_prices/labels, torch.ones_like(labels))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AbsolutePercentageLoss(nn.Module):\n    def __init__(self):\n        super(AbsolutePercentageLoss, self).__init__()\n    \n    # outputs the MC_STEPS predictions, labels the TOT_OPTIONS labels\n    def forward(self, outputs, labels, strike, is_call, coordinates):\n        mc_steps = len(coordinates)\n        loss = 0\n        mc_integral = 0\n        payoff = lambda  j :  torch.max(torch.zeros(len(outputs)), coordinates - strike[j] ) if is_call[j] else torch.max(torch.zeros(len(outputs)), strike[j] - coordinates)\n        densities = 1/INTERVAL_LENGTH \n        for j in range(len(labels)):\n          mc_price = payoff(j) * outputs / densities\n          #print(\"true price \" + str(labels[j]) )\n          #print(\"predicted price \" + str(mc_price.sum() / mc_steps) )\n          #print(\"absolute difference \" + str(torch.abs(labels[j] -  mc_price.sum() / mc_steps)))\n          print(\"delta (percentage of true price)\" + str(torch.abs(labels[j] -  mc_price.sum() / mc_steps)/labels[j]) )\n          loss += torch.abs(labels[j] -  mc_price.sum() / mc_steps)/labels[j]\n        return loss  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_encoding_net = TimeEncodingNet(64)\nimproved_model = ImprovedNet(time_encoding_net)\noptimizer_whole_network = torch.optim.Adam(improved_model.parameters(), lr=1e-4, weight_decay=1e-20)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_improved_model(input_model, optimizer, loss_fn, C1, C2, target_dataloader, pretrain_datasets, pretrain_iterations=3, pretrain_epochs = 5, equidistant_window=False):\n    mc_steps = 256 #fixed for pretraining\n    for _ in range(pretrain_iterations):\n        for dataset in pretrain_datasets:\n            prices_and_strikes = [ (dataset['SettlementPrice'].iloc[i], dataset['StrikePrice'].iloc[i], dataset['Putcall'].iloc[i]) for i in range(len(dataset))]\n            train_set_size = len(prices_and_strikes)\n            batch_size = train_set_size\n            target_dataloader = DataLoader(prices_and_strikes , batch_size=batch_size, shuffle = True)\n            for epoch in pretrain_epochs:\n                X = torch.zeros(mc_steps, 1).to(device)\n                X.uniform_(START_UNIF, INTERVAL_LENGTH)\n                pred = input_model(X)\n                loss = loss_fn(pred.cpu().squeeze(), target_prices, strike_prices, is_call, X.cpu().squeeze())\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(input_model.parameters(), 1e-1)\n                optimizer.step()\n    \n    for param in input_model.time_encoding_net.parameters(): \n        param.requires_grad = False #freeze layers of the time encoding net\n    \n    loss_history = []\n    for epoch in tqdm(range(int(600*C1))):\n        avg_loss = 0\n        for batch, (target_prices, strike_prices, is_call) in enumerate(target_dataloader):\n            if epoch < 100 * C1:\n                mc_steps = int(128*C2)\n            elif epoch < 200 * C1:\n                mc_steps = int(256*C2)\n            elif epoch < 250 * C1:\n                mc_steps = int(512*C2)\n            elif epoch < 275 * C1:\n                mc_steps = int(1024*C2)\n            elif epoch < 287 * C1:\n                mc_steps = int(2048*C2)\n            elif epoch < 293*C1:\n                mc_steps = int(4096*C2)\n            else:\n                mc_steps = int(8192*C2)\n            \n            \n            if equidistant_window:\n                coordinates=np.linspace(start=START_UNIF+INTERVAL_LENGTH/(2*mc_steps),stop=START_UNIF+INTERVAL_LENGTH-INTERVAL_LENGTH/(2*mc_steps),num=mc_steps)+np.random.uniform(low=-INTERVAL_LENGTH/(2*mc_steps),high=INTERVAL_LENGTH/(2*mc_steps),size=mc_steps)\n                coordinates = coordinates.reshape(  -1, 1)\n                X = torch.tensor(coordinates, dtype = torch.float32).to(device)\n                \n\n            else:\n                X = torch.zeros(mc_steps, 1).to(device)\n                X.uniform_(START_UNIF, INTERVAL_LENGTH)\n\n\n\n            #print(X)\n            #X.log_normal_(PROPOSAL_MU, PROPOSAL_SIGMA )\n            #print(X)\n            pred = input_model(X)\n            loss = loss_fn(pred.cpu().squeeze(), target_prices, strike_prices, is_call, X.cpu().squeeze())\n            \n            # Backpropagation\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(input_model.parameters(), 1e-1)\n            optimizer.step()\n            avg_loss += loss\n        loss_history.append(float(avg_loss/batch))\n    return loss_history\n\n","metadata":{},"execution_count":null,"outputs":[]}]}